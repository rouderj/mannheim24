---
title: "Lecture 3: Bayes + LVM/FA"
author: Jeff Rouder
date: June, 2024

output:
  beamer_presentation:
    theme: "metropolis"
    fonttheme: "structurebold"
    fig_caption: false
    incremental: false
header-includes   :
    - \usepackage{bm}
    - \usepackage{amsmath}
    - \usepackage{pcl}
---

```{r,echo=F,eval=T,warning=F,message=F}
knitr::opts_chunk$set(echo = FALSE,message=FALSE, warning=FALSE)
```


## Why Bayes For FA and SEM?

- lavaan is quick, easy, seamless, well documented
- you certainly don't need me here to show you how to use it
- And Bayes FA/SEM
  + not quick
  + not easy
  + not seamless
  + not well documented
  
## Why Bayes For FA and SEM

1. Fixes Heywood cases.
  + Variances are negative
  + Correlations are bigger than 1.0 or negative
  
## Why Bayes For FA and SEM

2. Most of our data are not cute little matrices $\bfY_{[I\times J]}$
3. Preprocessing the data to make $\bfY$
  + could be simple, like aggregation
  + could be complicated, like deriving a drift rate in a diffusion model
  + resulting $\bfY$ in real data is often too noisy to support FA/SEM
4. Needed: a fully integrated approach where FA affects preprocessing and preprocessing affects FA.
5. Enter Bayes


## Libraries

```{r,echo=T}
library(corrplot)
library(mvtnorm)
library(R2jags)
library(infinitefactor)
library(abind)
```
## Bayes For Cute Little Score Matrices

Here is our data set:

```{r,echo=T}
set.seed(123)
I=200
J=8
D=2
lambda=matrix(nrow=J,ncol=D)
lambda[,1]=seq(1,0,length=J)
lambda[,2]=seq(0,1,length=J)
Sigma=crossprod(t(lambda))+diag(rep(1^2,J))
y=rmvnorm(I,rep(0,J),Sigma)
```

## Your turn

- Program up a Bayes sampler in JAGS or stan (I will do JAGS) for recovering lambda

- Use the conditional formulation 
  + write out the model
  + implement it in JAGS or stan
  + run it and see if you can document issues

- When I program up all Gibbs steps, I get a lot of autocorrelation.  

## My turn

```{r}
set.seed(1234)
source('jagsLib.R')
prior=list("rDel"=.50,"rLam"=.50,a=0,b=1)
outCM=runCondMan(y,prior=prior)
phi=outCM$BUGSoutput$sims.list$phi
plot(phi[,10,2],typ='l')
acf(phi[,100,1])
plot(apply(phi,2:3,mean))
lambda=outCM$BUGSoutput$sims.list$lambda
plot(apply(lambda,2:3,mean))
hist(lambda[,4,1])
```

## Lambda [4,1]

```{r}
par(cex=1.5,mfrow=c(1,2))
plot(lambda[,4,1],typ='p',ylim=c(-1,1))
hist(lambda[,4,1],xlim=c(-1,1))
```


## Rotations!

- each iteration is corresponding to a different rotation.
- what to do?
- old way, fix loading to lower triangle

## Lower Triangle for 3 Factors
```{r}
table=matrix(nrow=J,ncol=3,"*")
table[1,1]=table[2,2]=table[3,3]="+"
table[1,2]=table[1,3]=table[2,3]=0
knitr::kable(table)
```

## Your Turn

Adapt your code for lower triangle.

```{r,cache=T}
out=runCondManL(y,prior=prior)
lambda=out$BUGSoutput$sims.list$lambda
plot(apply(lambda,2:3,mean))
```


## Lower Triangle, Lambda [4,1]

```{r}
par(cex=1.5,mfrow=c(1,2))
plot(lambda[,4,1],typ='p',ylim=c(-1,1))
hist(lambda[,4,1],xlim=c(-1,1))
```

## Post-Sampling Rotations

- Very new approach
- Align each iteration to a common rotation after the fact.
- Papastamoulis, P., & Ntzoufras, I. (2022). On the identifiability of Bayesian factor analytic models. Statistics and Computing, 32(2), 23. doi:10.1007/s11222-022-10084-4
- Poworoznek, E., Ferrari, F., & Dunson, D. (2021, July 29). Efficiently resolving rotational ambiguity in Bayesian matrix sampling with matching. Retrieved November 21, 2023, from http://arxiv.org/abs/2107.13783

##

```{r,echo=T}
phi=outCM$BUGSoutput$sims.list$phi
lambda=outCM$BUGSoutput$sims.list$lambda
M=dim(lambda)[1]
lambdaList=lapply(1:M,function(x) lambda[x,,])
etaList=lapply(1:M,function(x) phi[x,,])
aligned=jointRot(lambda = lambdaList, eta = etaList)
lambda=aperm(abind(aligned$lambda, along=3),c(3,1,2))
phi=aperm(abind(aligned$eta, along=3),c(3,1,2))
```

## Joint Rotation, Lambda [4,1]

```{r}
par(cex=1.5,mfrow=c(1,2))
plot(lambda[,4,1],typ='p',ylim=c(-1,1))
hist(lambda[,4,1],xlim=c(-1,1))
```


## All Lambdas
```{r}
plot(apply(lambda,2:3,mean))
```

## Make Positive

```{r,echo=T}
makePositive=function(lambda){
  dimMean=apply(lambda,3,mean)
  D=length(dimMean)
  for (i in 1:D) lambda[,,i]=sign(dimMean[i])*lambda[,,i]
  return(lambda)}
```

## 

Me? I use `jointRot` in `infinitefactor`

##  Hierarchical Factor Models

Illusions Data:

- 138 people
- 10 tasks
  +  5 Illusions
  + 2 versions each
- 15 trials per task

Q? How many factors mediate visual illusions.


## Model 1, Exploratory Factor Model
\[\begin{aligned}
Y_{ijk} | \theta_{ij} &\sim \mbox{N}(\theta_{ij},\tau^2_j)\\
\bftheta_i &\sim \mbox{N}_J(\bfmu,\;\bflambda\bflambda'+D(\bfdelta^2))
\end{aligned}
\]

## Model 2, Confirmatory Factor Model
\[
\bflambda=\begin{pmatrix}
\lambda_{11} & 0 & 0 & 0 & 0 & \lambda_{16} & \lambda_{17}\\
\lambda_{21} & 0 & 0 & 0 & 0 & \lambda_{26} & \lambda_{27}\\
0 &\lambda_{32}  & 0 & 0 & 0 & \lambda_{36} & \lambda_{37}\\
0 &\lambda_{42}  & 0 & 0 & 0 & \lambda_{46} & \lambda_{47}\\
0 & 0 &\lambda_{53} & 0 & 0 & \lambda_{56} & \lambda_{57}\\
0 & 0 &\lambda_{63} & 0 & 0 & \lambda_{66} & \lambda_{67}\\
0 & 0 & 0 & \lambda_{74} &0 & \lambda_{76} & \lambda_{77}\\
0 & 0 & 0 & \lambda_{84} &0 & \lambda_{86} & \lambda_{87}\\
0 & 0 & 0 & 0 & \lambda_{9,5}&\lambda_{96} & \lambda_{97}\\
0 & 0 & 0 & 0 & \lambda_{10,5}&\lambda_{10,6} & \lambda_{10,7}\\
\end{pmatrix}
\]

